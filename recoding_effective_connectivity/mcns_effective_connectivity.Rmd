---
title: "mcns_effective_connectivity"
output: html_document
---

Imports
```{r}
library(malecns)
library(coconatfly)
library(dplyr)
library(tidyr)
library(ggplot2)
library(plotly)
library(stringr)
library(igraph)
library(fafbseg)
library(tidygraph)
library(ggraph)
library(jsonlite)
```

Load metadata all cells
```{r}
synapse_threshold=5
mba<-mcns_body_annotations()
mba.type.coalesced <-  mba %>%
  mutate(type = coalesce(type, 
                         flywire_type, 
                         hemibrain_type, 
                         manc_type),
         top_nt=coalesce(consensus_nt,celltype_predicted_nt,predicted_nt))
#flytable<-flytable_query("select * from info",limit=1000) #this throws an error right now?
mba_synonyms <- mba%>%
  select(bodyid,type,flywire_type,synonyms)%>%
  filter(!is.na(flywire_type))
mba_synonyms_unique <- mba_synonyms%>%distinct(type,.keep_all=T)

#data<-fromJSON('/Users/fkampf/mappings.json')

```


Starter type/s or cell/s
```{r}
starter = c('JO-B','(JO-B)')
drop_hemispheres <- F
```


Function
```{r}
generate_mcns_output <- function(starter = c("JO-B", "(JO-B)"),
                                 drop_hemispheres = FALSE,
                                 synapse_threshold = 5,
                                 norm_cutoff = 0.02) {
  
  #' Generate Underlying Connectivity Data for Effective Connectivity Explorations
  #'
  #' This function generates a summarized version of MCNS output data from partner connections,
  #' intended to provide the underlying connectivity data for effective connectivity explorations.
  #' It collects MCNS IDs based on the provided starter values, retrieves partner connections using a
  #' synapse threshold, and joins with additional type information. The data is then grouped and summarized,
  #' and the pre-synaptic weights are normalized. Only groups with a normalized weight above a given cutoff
  #' are retained.
  #'
  #' @param starter A character vector of starter IDs. Default is `c("JO-B", "(JO-B)")`.
  #' @param drop_hemispheres Logical indicating whether to group without hemisphere information.
  #'   If TRUE, the groups will be defined only by `pre_type` and `type`; if FALSE, additional grouping by
  #'   `soma_side_pre` and `somaSide` is performed. Default is `FALSE`.
  #' @param synapse_threshold Numeric synapse threshold for selecting partner connections.
  #'   Default is `5`.
  #' @param norm_cutoff Numeric value specifying the cutoff for the normalized pre-synaptic weight.
  #'   Groups with `pre_normed_weight` below this cutoff will be filtered out. Default is `0.02`.
  #'
  #' @return A list with two elements:
  #' \describe{
  #'   \item{output}{A data.frame/tibble with the summarized and filtered MCNS output.}
  #'   \item{unique_post_ids}{A numeric vector with the unique post IDs from the final output.}
  #' }
  #'
  #' @examples
  #' \dontrun{
  #'   result <- generate_mcns_output()
  #'   head(result$output)
  #'   print(result$unique_post_ids)
  #' }
  #'
  ##############################################################################
  
  # Helper function to sum synapses for a given query_list.
  query_total_synapses <- function(query_list, query_df) {
    sum_synapses <- query_df %>%
      filter(bodyid %in% query_list) %>%
      pull(weight) %>%
      sum()
    return(sum_synapses)
  }
  
  #get cf_ids
  job_mcns_id <- do.call(c, lapply(starter, function(x) cf_ids(malecns = x)))

  
  # Join with additional type information and remove duplicates.
  job.prout.mcns.output.0 <-   cf_partners(job_mcns_id, partners = "out", threshold = synapse_threshold)%>%
    left_join(mba.type.coalesced %>%
                dplyr::select(bodyid, type, soma_side) %>%
                dplyr::rename(pre_type = type, soma_side_pre = soma_side),
              by = "bodyid") %>%
    dplyr::distinct()
  
  # Print the number of unique post_id values before grouping/filtering.
  print(length(job.prout.mcns.output.0 %>% pull(post_id) %>% unique()))
  
  # Group and summarize the data.
  if (drop_hemispheres) {
    job.prout.mcns.output.1 <- job.prout.mcns.output.0 %>%
      group_by(pre_type, type) %>%
      summarise(
        pre_id_list = I(list(unique(pre_id))),
        post_id_list = I(list(unique(post_id))),
        weight = sum(weight),
        dataset = first(dataset),
        .groups = "drop"
      ) %>%
      rename(post_type = type)
  } else {
    job.prout.mcns.output.1 <- job.prout.mcns.output.0 %>%
      group_by(pre_type, type, soma_side_pre, somaSide) %>%
      summarise(
        pre_id_list = I(list(unique(pre_id))),
        post_id_list = I(list(unique(post_id))),
        weight = sum(weight),
        dataset = first(dataset),
        .groups = "drop"
      ) %>%
      rename(post_type = type,
             soma_side_post = somaSide)
  }
  
  # Normalize by the total number of pre-synapses.
  job.prout.mcns.output.2 <- job.prout.mcns.output.1 %>%
    rowwise() %>%
    mutate(total_synapses_out = query_total_synapses(pre_id_list, job.prout.mcns.output.0)) %>%
    ungroup() %>%
    mutate(pre_normed_weight = weight / total_synapses_out) %>%
    filter(pre_normed_weight >= norm_cutoff)
  
  # Unpack post_id_list and get the unique post ids.
  unique_post_ids <- do.call(c, job.prout.mcns.output.2 %>% pull(post_id_list))
  
  # Return the filtered output and unique post IDs in a list.
  return(list(output = job.prout.mcns.output.2,
              unique_post_ids = unique_post_ids))
}
```



Raw example of how the function works and test edge cases with flat connectome
```{r}
generate_mcns_output_flat <- function(starter = c("JO-B", "(JO-B)"),
                                        drop_hemispheres = FALSE,
                                        use_fw_names = TRUE,
                                        synapse_threshold = 5,
                                        norm_cutoff = 0.02,
                                        path = "/Users/fkampf/Downloads/snapshots_2025-02-11-a4c0d9-unlocked_flat-connectome_connectome-weights-2025-02-11-a4c0d9-unlocked-minconf-0.5-primary-only.feather",
                                        debug = TRUE,
                                      just_load=FALSE) {
  
  library(dplyr)
  library(arrow)
  library(data.table)
  
  overall_start <- Sys.time()

  ## Step 1: Load the flat connectome dataset from Feather.
  t1 <- Sys.time()
  flat.connectome.feather <- arrow::open_dataset(path, format = "feather")
  flat.connectome.df.0 <- flat.connectome.feather %>%
    collect() %>%
    data.frame()%>%
    mutate(body_pre=as.character(body_pre),body_post=as.character(body_post))
  t2 <- Sys.time()
  if (debug) message("Dataset loaded. Elapsed time for loading: ",
                       round(as.numeric(difftime(t2, t1, units = "secs")), 2), " seconds")
  
  ## Step 2: Join additional type information for pre- and post-synaptic elements.
  t3 <- Sys.time()
  flat.connectome.df.1 <- flat.connectome.df.0 %>%
    left_join(mba.type.coalesced %>%
                select(bodyid, 
                       type, 
                       soma_side,
                       top_nt,
                       fru_dsx,
                       dimorphism,
                       flywire_type) %>%
                rename(pre_type = type, 
                              soma_side_pre = soma_side, 
                              pre_fru_dsx = fru_dsx, 
                              pre_dimorphism = dimorphism, 
                              pre_nt = top_nt,
                              pre_fw_type=flywire_type)%>%
                mutate(bodyid=as.character(bodyid)),
              by = c("body_pre" = "bodyid")) %>%
    select(-type_pre) %>%
    left_join(mba.type.coalesced %>%
                select(bodyid, 
                       type, 
                       soma_side,
                       top_nt,
                       fru_dsx,
                       dimorphism,
                       flywire_type) %>%
                rename(post_type = type, 
                       soma_side_post = soma_side, 
                       post_fru_dsx = fru_dsx, 
                       post_dimorphism = dimorphism, 
                       post_nt = top_nt,
                       post_fw_type=flywire_type)%>%
                mutate(bodyid=as.character(bodyid)),
              by = c("body_post" = "bodyid")) %>%
    select(-type_post) %>%
    rename(pre_id = body_pre, post_id = body_post) %>%
    filter(weight >= synapse_threshold)
  t4 <- Sys.time()
  if (debug) message("Type info joined. Elapsed time for join: ",
                       round(as.numeric(difftime(t4, t3, units = "secs")), 2), " seconds")
  
  ## Step 3: Filter rows by pre-synaptic type.
  t5 <- Sys.time()
  flat.connectome.df.2 <- flat.connectome.df.1 %>%
    filter(pre_type %in% starter | pre_id %in% starter)
  t6 <- Sys.time()
  if (debug) message("Filtering complete. Elapsed time for filtering: ",
                       round(as.numeric(difftime(t6, t5, units = "secs")), 2), " seconds")
  
  ## (Optional) Print the number of unique post IDs in the filtered dataset.
  num_unique_post_ids <- length(flat.connectome.df.2 %>% pull(post_id) %>% unique())

  ## Step 4: Group and summarize the data using data.table for fast grouping.
  t7 <- Sys.time()
  
  majority_vote <- function(x) {
  x <- x[!is.na(x)]
  if (length(x) == 0) return(NA_character_)
  tab <- table(x)
  return(names(tab)[which.max(tab)])
}
  

if (use_fw_names){
  flat.connectome.df.2<- flat.connectome.df.2%>%
    mutate(pre_type=coalesce(pre_fw_type,pre_type),post_type=coalesce(post_fw_type,post_type))
} else {
  flat.connectome.df.2<- flat.connectome.df.2%>%
    mutate(pre_type=coalesce(pre_type,pre_fw_type),post_type=coalesce(post_type,post_fw_type))
}
dt_df2 <- as.data.table(flat.connectome.df.2)


if (drop_hemispheres) {
  job.prout.mcns.output.1 <- dt_df2[, .(
    pre_id_list    = I(list(unique(pre_id))),
    post_id_list   = I(list(unique(post_id))),
    weight         = sum(weight),
    post_fru_dsx   = majority_vote(post_fru_dsx),
    post_dimorphism= majority_vote(post_dimorphism),
    post_nt        = majority_vote(post_nt),
    pre_fru_dsx    = majority_vote(pre_fru_dsx),
    pre_dimorphism = majority_vote(pre_dimorphism),
    pre_nt         = majority_vote(pre_nt),
    post_fw_type   = majority_vote(post_fw_type),
    pre_fw_type    = majority_vote(pre_fw_type)
  ), by = .(pre_type, post_type)]
} else {
  job.prout.mcns.output.1 <- dt_df2[, .(
    pre_id_list    = I(list(unique(pre_id))),
    post_id_list   = I(list(unique(post_id))),
    weight         = sum(weight),
    post_fru_dsx   = majority_vote(post_fru_dsx),
    post_dimorphism= majority_vote(post_dimorphism),
    post_nt        = majority_vote(post_nt),
    pre_fru_dsx    = majority_vote(pre_fru_dsx),
    pre_dimorphism = majority_vote(pre_dimorphism),
    pre_nt         = majority_vote(pre_nt),
    post_fw_type   = majority_vote(post_fw_type),
    pre_fw_type    = majority_vote(pre_fw_type)
  ), by = .(pre_type, post_type, soma_side_pre, soma_side_post)]
}#todo maybe group here by fw type or something to get fw types
job.prout.mcns.output.1 <- as.data.frame(job.prout.mcns.output.1)

  t8 <- Sys.time()
  if (debug) message("Grouping done using data.table. Elapsed time for grouping: ",
                       round(as.numeric(difftime(t8, t7, units = "secs")), 2), " seconds")
  if(!just_load){
  ## Step 5: Precompute lookup tables for total synapse weights.
  t9 <- Sys.time()
  lookup_pre <- flat.connectome.df.1 %>%
    group_by(pre_id) %>%
    summarise(total_pre = sum(weight), .groups = "drop")
  
  lookup_post <- flat.connectome.df.1 %>%
    group_by(post_id) %>%
    summarise(total_post = sum(weight), .groups = "drop")
  # Create named vectors for fast lookup.
  lookup_pre_vec <- setNames(lookup_pre$total_pre, lookup_pre$pre_id)
  lookup_post_vec <- setNames(lookup_post$total_post, lookup_post$post_id)
  t10 <- Sys.time()
  if (debug) message("Lookup tables computed. Elapsed time: ",round(as.numeric(difftime(t10, t9, units = "secs")), 2), " seconds")
  
  ## Step 6: Compute normalized weights using vectorized lookups.

  job.prout.mcns.output.2 <- job.prout.mcns.output.1 %>%
    mutate(total_synapses_out = vapply(pre_id_list, function(ids) {
      sum(as.numeric(lookup_pre_vec[ids]), na.rm = TRUE)
    }, numeric(1)),
    total_synapses_in = vapply(post_id_list, function(ids) {
      sum(as.numeric(lookup_post_vec[ids]), na.rm = TRUE)
    }, numeric(1))) %>%
    mutate(pre_normed_weight = weight / total_synapses_out,
           post_normed_weight = weight / total_synapses_in) %>%
    filter(pre_normed_weight >= norm_cutoff)
  t11 <- Sys.time()
  if (debug) message("Normalization done. Elapsed time for normalization: ",
                       round(as.numeric(difftime(t11, t10, units = "secs")), 2), " seconds")
  }else{
  job.prout.mcns.output.2<- job.prout.mcns.output.1
  }

  t11 <- Sys.time()
  if (debug) {
    message("Total elapsed time: ", round(as.numeric(difftime(t11, overall_start, units = "secs")), 2), " seconds")

  }
  
  return(job.prout.mcns.output.2)
}
```


Function to unpack unique post ids
```{r}
unpack_unique_post_ids <- function(df) {
  dt <- as.data.table(df)
  unique_post_ids <- unique(dt[, unlist(post_id_list, use.names = FALSE)])
  return(unique_post_ids)
}
```



Run function for 4 JO-B levels

```{r}

drop_hemispheres <- F
use_fw_names <-T
just_load=T
#lvl0
lvl0.prout.mcns.output <- generate_mcns_output_flat(starter = c("JO-B", "(JO-B)"),
                               drop_hemispheres = drop_hemispheres,
                               synapse_threshold = 5,
                               norm_cutoff = 0.00,
                               use_fw_names=use_fw_names,
                               just_load=just_load)
lvl0.unique.post.ids = unpack_unique_post_ids(lvl0.prout.mcns.output)
lvl0.prout.mcns.output$level = 0

#lvl1
lvl1.prout.mcns.output <- generate_mcns_output_flat(lvl0.unique.post.ids,
                               drop_hemispheres = drop_hemispheres,
                               synapse_threshold = 5,
                               norm_cutoff = 0.00,
                               use_fw_names=use_fw_names,
                               just_load=just_load)
lvl1.unique.post.ids = unpack_unique_post_ids(lvl1.prout.mcns.output)
lvl1.prout.mcns.output$level = 1

#lvl2
lvl2.prout.mcns.output <- generate_mcns_output_flat(lvl1.unique.post.ids,
                               drop_hemispheres = drop_hemispheres,
                               synapse_threshold = 5,
                               norm_cutoff = 0.00,
                               use_fw_names=use_fw_names,
                               just_load=just_load)
lvl2.unique.post.ids = unpack_unique_post_ids(lvl2.prout.mcns.output)
lvl2.prout.mcns.output$level = 2

#lvl3
lvl3.prout.mcns.output <- generate_mcns_output_flat(lvl2.unique.post.ids,
                               drop_hemispheres = drop_hemispheres,
                               synapse_threshold = 5,
                               norm_cutoff = 0.00,
                               use_fw_names=use_fw_names,
                               just_load=just_load)
lvl3.unique.post.ids = unpack_unique_post_ids(lvl3.prout.mcns.output)
lvl3.prout.mcns.output$level = 3

#lvl4
lvl4.prout.mcns.output <- generate_mcns_output_flat(lvl3.unique.post.ids,
                               drop_hemispheres = drop_hemispheres,
                               synapse_threshold = 5,
                               norm_cutoff = 0.00,
                               use_fw_names=use_fw_names,
                               just_load=just_load)
lvl4.unique.post.ids = unpack_unique_post_ids(lvl4.prout.mcns.output)
lvl4.prout.mcns.output$level = 4
```


postprocessing 
```{r}




all.levels.prout.mcns.output <- rbind(lvl0.prout.mcns.output,
                                      lvl1.prout.mcns.output,
                                      lvl2.prout.mcns.output,
                                      lvl3.prout.mcns.output,
                                      lvl4.prout.mcns.output)%>%
  distinct()
all.levels.prout.mcns.output <-all.levels.prout.mcns.output%>%filter(!is.na(pre_type),!is.na(post_type))
#alternative add side to types
try(all.levels.prout.mcns.output <- all.levels.prout.mcns.output %>%
  mutate(pre_type=paste0(pre_type,'_',soma_side_pre))%>%
  mutate(post_type=paste0(post_type,'_',soma_side_post)),silent=T)


```

Complete adjacency matrix
```{r}
path = "/Users/fkampf/Downloads/snapshots_2025-02-11-a4c0d9-unlocked_flat-connectome_connectome-weights-2025-02-11-a4c0d9-unlocked-minconf-0.5-primary-only.feather"
flat.connectome.feather <- arrow::open_dataset(path, format = "feather")
flat.connectome.df.0 <- flat.connectome.feather %>%
    collect() %>%
    data.frame()%>%
    mutate(body_pre=as.character(body_pre),body_post=as.character(body_post))

type2id.dict.0 <- flat.connectome.df.0%>%
  select(body_pre,type_pre)%>%
  distinct()%>%
  rename(body=body_pre,type=type_pre)

type2id.dict.1 <- flat.connectome.df.0%>%
  select(body_post,type_post)%>%
  distinct()%>%
  rename(body=body_post,type=type_post)

type2id.dict.final <- rbind(body2id_map.1,body2id_map.0)%>%distinct()

unique_body <- union(flat.connectome.df.0$body_pre, flat.connectome.df.0$body_post)

id2type.map <- type2id.dict.final$type[match(unique_body, type2id.dict.final$body)]

adj_matrix_full <- sparseMatrix(
  i = match(flat.connectome.df.0$body_pre, unique_body),
  j = match(flat.connectome.df.0$body_post, unique_body),
  x = flat.connectome.df.0$weight,  # or use edge_df$syn_count or other weight column if available
  dims = c(length(unique_body), length(unique_body)),
  dimnames = list(unique_body, unique_body)
)
```

#Reconstruct circuit using the full adjacency matrix
```{r}
indices <- which(id2type.map == "(JO-B)")
sub_mat <- adj_matrix_full[indices, , drop = FALSE]  # retain sparse structure

# For a dgCMatrix, diff(sub_mat@p) gives the number of nonzeros per column.
nonzero_cols <- which(diff(sub_mat@p) > 0)


```





Adjacency shenanigans
```{r}

library(dplyr)
library(Matrix)
library(igraph)

# Combine levels and remove duplicates
all.levels.prout.mcns.output <- rbind(lvl0.prout.mcns.output,
                                      lvl1.prout.mcns.output,
                                      lvl2.prout.mcns.output,
                                      lvl3.prout.mcns.output,
                                      lvl4.prout.mcns.output) %>%
  distinct()%>%
  filter(weight>=5)
  #%>%group_by(pre_type, post_type) %>%
  #slice_min(order_by = level, with_ties = FALSE) %>%
  #ungroup()%>%
  #filter(!is.na(pre_type),!is.na(post_type))

neuron_types <- union(all.levels.prout.mcns.output$pre_type, all.levels.prout.mcns.output$post_type)

adj_matrix <- sparseMatrix(
  i = match(all.levels.prout.mcns.output$pre_type, neuron_types),
  j = match(all.levels.prout.mcns.output$post_type, neuron_types),
  x = all.levels.prout.mcns.output$weight,  # or use edge_df$syn_count or other weight column if available
  dims = c(length(neuron_types), length(neuron_types)),
  dimnames = list(neuron_types, neuron_types)
)


colScale <- function(A, na.rm = TRUE) {
  scalefac <- 1 / Matrix::colSums(A)
  if (na.rm) scalefac[!is.finite(scalefac)] <- 0
  B <- A %*% Matrix::Diagonal(x = scalefac)
  B
  
}

rowScale <- function(A, na.rm = TRUE) {
  scalefac <- 1 / Matrix::rowSums(A)
  if (na.rm) scalefac[!is.finite(scalefac)] <- 0
  B <- Matrix::Diagonal(x = scalefac) %*% A
  B
}

adj_matrix_per_in <- colScale(adj_matrix)
colnames(adj_matrix_per_in) <- colnames(adj_matrix)
rownames(adj_matrix_per_in) <- rownames(adj_matrix)
adj_matrix_thresholded <- adj_matrix_per_in

adj_matrix_thresholded[adj_matrix_thresholded < 0.01] <- 0
adj_matrix[adj_matrix_thresholded < 0.01] <- 0
g.prout.mcns <- graph_from_adjacency_matrix(
  adj_matrix_thresholded,
  mode = "directed",
  weighted = TRUE,
  diag = FALSE
)

```




path finding new


```{r}

# Define parameters for path search
n_paths <- 3           # Number of strongest paths to find for each start-target pair
diversity <- 'othernodes'  # othernodes,otheredges,none


library(ggplot2)
library(reshape2)

# Your existing code
cell_types <- c('CB1076','CB1078', 'CB3710', 'CB2521','CB1542', 'CB1038', 'CB1427', 'CB2556a', 'CB2380') #all
cell_types <- c('(JO-B)') #only JO-B
#cell_types <- c('CB1542','CB1078')

target_cells <- c("vpoEN",'CB1385',"vpoEN",'CB1385')  # List of target cells
target_cells <- c(target_cells,'pMP2')
target_cells <- c(target_cells,mba%>% filter(grepl('aSP-k',synonyms))%>%filter(!is.na(type))%>%pull(type)%>%unique())

for (tc in target_cells){
  for (lr in c('_L','_R','_NA')){
    target_cells<-c(target_cells,paste0(tc,lr))
    }
}

for (ct in cell_types){
  for (lr in c('_L','_R','_NA')){
    cell_types<-c(cell_types,paste0(ct,lr))
    }
}

#target_cells <- c(target_cells,V(g.prout.mcns)$name[grepl("P1_", V(g.prout.mcns)$name)])
#target_cells <- c(all.levels.prout.mcns.output%>%filter((post_type=='vpoEN')&(weight>=0.1))%>%pull(pre_type)%>%unique(),target_cells)

# Transform weights using log
E(g.prout.mcns)$log_weight <- -log(E(g.prout.mcns)$weight)  # Negate to convert to shortest path problem



# Initialize an empty list to store results and a vector for important nodes
results <- list()
all_important_nodes <- character()

# Loop over all start (cell_types) and target (target_cells) pairs
for (start in cell_types) {
  for (end in target_cells) {

    # Only process if both start and end exist in the graph
    if ((start %in% V(g.prout.mcns)$name) && (end %in% V(g.prout.mcns)$name)) {

      # Make a copy of the graph to modify iteratively
      g_temp <- g.prout.mcns
      
      # List to hold all paths for this start–end pair
      pair_paths <- list()
      
      # Find up to 'n_paths' for the given pair
      for (i in 1:n_paths) {
        # Compute shortest path (using negative log weights so that lower is stronger)
        path_result <- shortest_paths(g_temp, from = start, to = end,
                                      weights = E(g_temp)$log_weight,
                                      output = "both")
        
        # Check if a path was found
        if (length(path_result$vpath[[1]]) > 0) {
          path_nodes <- path_result$vpath[[1]]
          path_edges <- path_result$epath[[1]]
          # Convert back to strength: reverse the -log transform
          final_strength <- exp(-sum(E(g_temp)[path_edges]$log_weight))
          
          # Save the results; note that we display the path using the names from the original graph
          pair_paths[[paste0("path", i)]] <- list(
            start = start,
            end = end,
            path = paste(V(g.prout.mcns)[path_nodes]$name, collapse = " -> "),
            strength = final_strength
          )
          # Collect nodes from this path into the overall list
          all_important_nodes <- c(all_important_nodes, V(g.prout.mcns)[path_nodes]$name)
          
          # Modify the temporary graph to seek a different path next time
          if (diversity=='othernodes') {
            # Remove intermediate nodes (all nodes except the start and end)
            intermediate_nodes <- V(g.prout.mcns)[path_nodes]$name  # get names from the original graph
            nodes_to_remove <- setdiff(intermediate_nodes, c(start, end))
            if (length(nodes_to_remove) > 0) {
              g_temp <- delete_vertices(g_temp, nodes_to_remove)
            }
          } else if(diversity=='otheredges'){
            # Remove the edges used in this path so that the next path will be different
            g_temp <- delete_edges(g_temp, path_edges)
          }else{
            1+1
          }
            
        } else {
          # If no further path is found, break out of the inner loop
          break
        }
      }
      # Save all the paths found for this start–end pair into the results list
      results[[paste(start, end, sep = "->")]] <- pair_paths
    } else {
      # If either node doesn't exist, store an NA entry for this pair
      results[[paste(start, end, sep = "->")]] <- list(list(
        start = start,
        end = end,
        path = NA,
        strength = NA
      ))
    }
  }
}

# Flatten the nested results list into a single data frame for easier inspection:
results_df <- do.call(rbind, lapply(results, function(pair_list) {
  do.call(rbind, lapply(pair_list, function(r) {
    data.frame(
      start = r$start,
      end = r$end,
      path = r$path,
      strength = r$strength,
      stringsAsFactors = FALSE
    )
  }))
}))

# (Optional) Get unique important nodes encountered in all the paths:
all_important_nodes <- unique(all_important_nodes)
results_df_wo_na <- results_df%>%
  filter(!is.na(strength))
# Look at the results
print(results_df_wo_na)
```





Open graph in RCy3

```{r}
library(RCy3)
library(dplyr)


nodes_to_keep <- c("JO-B","JO-B",'CB1078','CB1542','CB2108','CB1383','CB1066,CB3649
','CB2364','CB3382','PVLP033','CB3162','CB3382','vpoEN',V(g.prout.mcns)$name[grepl("P1_", V(g.prout.mcns)$name)],
'AVLP721m','AVLP299_c','DNp55','CB1385','AVLP711m','SIP108m','SIP120m','SIP116m','SIP114m','PVLP211m','pMP2','AVLP744m')
nodes_to_keep <- unique(all_important_nodes)

#add P1 only to neurons which would be in the graph anyway
P1_2_add <- all.levels.prout.mcns.output%>%
  filter(pre_type %in% nodes_to_keep,
         grepl("^P1_", post_type))%>%
  pull(post_type)%>%
  unique()

#nodes_to_keep<-c(nodes_to_keep,P1_2_add)

#cleaning up graph and subsampling
g.sub.prout.mcns <- induced_subgraph(g.prout.mcns, vids = V(g.prout.mcns)[name %in% nodes_to_keep])


nodes_to_filter <- V(g.sub.prout.mcns)[grepl("^P1_", name)]
try(edges_to_delete <- incident(g.sub.prout.mcns, nodes_to_filter, mode = "out"),silent=T)
try(g.sub.prout.mcns <- delete_edges(g.sub.prout.mcns, edges_to_delete),silent=T)



node_names <- V(g.sub.prout.mcns)$name
distance_to_JO_B <- shortest.paths(g.sub.prout.mcns, to = V(g.sub.prout.mcns)[name == "(JO-B)_NA"], weights = NA)
distance_to_JO_B <- shortest.paths(g.sub.prout.mcns, to = V(g.sub.prout.mcns)[name == "(JO-B)"], weights = NA)
V(g.sub.prout.mcns)$distance_to_JO_B <- ifelse(node_names == "(JO-B)_NA", 0, as.integer(distance_to_JO_B))
#V(g.sub.prout.mcns)$distance_to_JO_B <- ifelse(node_names == "(JO-B)", 0, as.integer(distance_to_JO_B))

fru_dsx_values <- all.levels.prout.mcns.output %>%
    group_by(pre_type,post_type) %>%
    mutate(pre_fru_dsx = !is.na(pre_fru_dsx),
           post_fru_dsx = !is.na(post_fru_dsx))%>%
    summarize(pre_fru_dsx = first(pre_fru_dsx),
              post_fru_dsx = first(post_fru_dsx)) %>%
    ungroup()%>%
    pivot_longer(
        cols = c(pre_fru_dsx, post_fru_dsx, pre_type, post_type),
        names_to = c("side",".value"),  # .value tells pivot_longer to use part of the name as the output column name
        names_sep = "_"                # splits the column names at the underscore
    ) %>%select(-side)%>%distinct()

lvl_values <- all.levels.prout.mcns.output %>%
    group_by(pre_type) %>%
    summarize(level = min(level)) %>%
    ungroup()


nt_values <- all.levels.prout.mcns.output %>%
    group_by(pre_type,post_type) %>%
    mutate(pre_nt = first(pre_nt),
           post_nt = first(post_nt))%>%
    summarize(pre_nt = first(pre_nt),
              post_nt = first(post_nt)) %>%
    ungroup()%>%
    pivot_longer(
        cols = c(pre_nt, post_nt, pre_type, post_type),
        names_to = c("side",".value"),  # .value tells pivot_longer to use part of the name as the output column name
        names_sep = "_"                # splits the column names at the underscore
    ) %>%select(-side)%>%distinct()


lvl_for_vertices <- lvl_values$level[match(V(g.sub.prout.mcns)$name, lvl_values$pre_type)]
nt_for_vertices <- nt_values$nt[match(V(g.sub.prout.mcns)$name, nt_values$type)]


fru_dsx_for_vertices <- fru_dsx_values$fru[match(V(g.sub.prout.mcns)$name, fru_dsx_values$type)]

V(g.sub.prout.mcns)$level <- lvl_for_vertices
V(g.sub.prout.mcns)$nt <- nt_for_vertices
V(g.sub.prout.mcns)$fru_dsx <- fru_dsx_for_vertices

el<- as_edgelist(g.sub.prout.mcns)
is_p1p1 <- grepl("^P1_", el[, 1])# & grepl("^P1_", el[, 2])
E(g.sub.prout.mcns)$is_p1_p1 <- is_p1p1

rename_dict <- list(
  "CB1078" = "aPN1/CB1078",
  "CB1542" = "aPN1/CB1542",
  "CB3382" = "aIP-g/CB3382",
  "CB2364" = "aSP-K/CB2364",
  'AVLP721m' = 'vPN1/AVLP721m',
  "AVLP299_c" = "aIP-b/AVLP299_c",
  "SIP114m" = "aSP-a/SIP114m",
  'SIP116m' = 'aSP-a/SIP116m',
  "SIP120m" = "aSP-a/SIP120m",
  "PVLP211m" = "pIP-e/PVLP211m",
  'AVLP744m' = 'pIP-e/AVLP744m',
  'SIP108m'='pIP-e/SIP108m',
  'AVLP711m'='pIP-e/AVLP711m',
  'CB1385' = 'vpoIN_CB1385',
  'AVLP755m'='AVLP755m/aSP-k/aSP8/LC1',
  'LHAV4c2'='LHAV4c2/aSP-k/aSP8/LC1',
  'AVLP734m'='AVLP734m/aSP-k/aSP8/LC1',
  'ICL008m'='ICL008m/aSP-k/aSP8/LC1',
  'AVLP760m'='AVLP760m/aSP-k/aSP8/LC1'
  
)


# Apply the dictionary to rename the vertices in the graph
V(g.sub.prout.mcns)$name <- sapply(V(g.sub.prout.mcns)$name, function(x) ifelse(x %in% names(rename_dict), rename_dict[[x]], x))




RCy3::createNetworkFromIgraph(g.sub.prout.mcns)


```
Minimum graph
```{r}
g.reduced <- simplify(
  g.sub.prout.mcns,
  remove.multiple = TRUE,
  remove.loops = TRUE,
  edge.attr.comb = list(pre_normed_weight = "max",
                        level = "min",
                        log_weight = "max",
                        is_p1_p1 = "first",
                        inv_weights = "min")  # Keep max weight if duplicates exist
)



RCy3::createNetworkFromIgraph(g.reduced)


```



